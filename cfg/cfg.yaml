global:
  model: 'WHAMSAHNet'
  device: 'cuda'
  weights_path: 'model/weights/whamsah_v7noact_ep_290_sdr_3-8244.pth'

separator:
  chunk: 65536 #this is the input chunk size for the model
  hop: 65536 #32768
  window: ''
  sr: 44100

model:
  targets: ['vocals']
  depth: 6
  growth: 2

audio:
  chunk_size: 485100
  num_channels: 2
  sample_rate: 44100
  min_mean_abs: 0.001
  segment: 11

training:
  batch_size: 4
  gradient_accumulation_steps: 8
  grad_clip: 0
  instruments:
  - vocals
  - bass
  - drums
  - other
  lr: 2.99e-05
  patience: 2
  reduce_factor: 0.95
  target_instrument: vocals #use null to have all of them
  num_epochs: 200
  num_steps: 25
  q: 0.95
  coarse_loss_clip: true
  ema_momentum: 0.999
  optimizer: adamw
  other_fix: false # it's needed for checking on multisong dataset if other is actually instrumental
  use_amp: false # enable or disable usage of mixed precision (float16) - usually it must be true
  #I am setting it to false until I can work around the issue of input being put as float16 automatically and 
  #weights staying float32
  recompute_val_mixture: False
  val_on_cpu: False 


augmentations:
  enable: true # enable or disable all augmentations (to fast disable if needed)
  loudness: true # randomly change loudness of each stem on the range (loudness_min; loudness_max)
  loudness_min: 0.5
  loudness_max: 1.5
  mixup: true # mix several stems of same type with some probability (only works for dataset types: 1, 2, 3)
  mixup_probs: !!python/tuple # 2 additional stems of the same type (1st with prob 0.2, 2nd with prob 0.02)
    - 0.2
    - 0.02
  mixup_loudness_min: 0.5
  mixup_loudness_max: 1.5


inference:
  batch_size: 1
  dim_t: 512
  num_overlap: 1

misc:
  norm_stage: false
  norm_config_path: 'configs/normalizer.yaml'
